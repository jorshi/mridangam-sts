# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.14.5
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---
# %%
import pytorch_lightning as pl
import torch

from mridangam.data import MridangamDataModule
from mridangam.models import MLP
from mridangam.tasks import MridangamTonicClassification

# %load_ext autoreload
# %autoreload 2

# %% [markdown]
# # Mridangam Tonic Classification
#
# For this first experiment I am using embeddings generated by Crepe with a simple
# deep MLP classifier.

# %% [markdown]
# The first step is to preprocess our dataset of audio files and create a PyTorch
# Lightning DataModule for our experiment. In this step we compute embeddings using
# the full Crepe model and save the result embeddings to disk.

# %%
datamodule = MridangamDataModule(
    dataset_dir="dataset/preprocesed",
    unprocessed_dir="dataset/mridangam_stroke_1.5/",
    batch_size=16,
    num_workers=0,
    attribute="tonic",
    max_files=100,
)
datamodule.prepare_data()

# %%
datamodule.setup("fit")
dataloader = datamodule.train_dataloader()

# %%
# Get input feature size and target num_classes from data
audio, embedding, label = next(iter(dataloader))

in_features = embedding.size(-1)
print(in_features)

out_features = dataloader.dataset.num_classes
print(out_features)

print(label.dtype)

# %%
mlp = MLP(in_features=in_features, hidden=[256, 128], out_features=out_features)
model = MridangamTonicClassification(model=mlp)

# %%
print(model)

# %%
trainer = pl.Trainer(max_epochs=1000)
trainer.fit(model=model, train_dataloaders=dataloader)

# %%
datamodule.setup("test")
test_dataloader = datamodule.test_dataloader()
trainer.test(model=model, dataloaders=test_dataloader)

# %%
print(len(test_dataloader.dataset))
audio, emb, label = test_dataloader.dataset[9]

print(label)
y = mlp(emb)
print(torch.argmax(y))

# %%
dataset = datamodule.train_dataset
audio, y = dataset[3]
audio.shape
print(dataset.num_classes)
